# Research Assistant NN Verification

__Neural Networks__ are quite used in many industries. There are many __research papers__ being published for __neural networks__. However, there can be outcomes from neural networks which lead to different output results as a result of the change in input. Therefore, verification tools are designed and implemented to specify the behavior of the neural network along with verifying it's use. 

I'm currently working under Tan Cheng for Neural Network verification and where I use state-of-the-art tools for building and deploying the neural networks in production. 
My research involves how changing and altering the neural networks leads to the change in outputs. We test whether a given neural network follows a defined specification. 

<img src = "https://github.com/suhasmaddali/Images/blob/main/Research%20Assistant%20Image.jpg" width = 750/>

One of the interesting applications of Neural Network Verification is in self-driving cars. In the case of motion planning and other activities, neural networks are used extensively. Attackers try to modify the outputs from the Neural Networks by applying a distorted version of the input. As a result, the output from the Neural Network is different from what is actually expected from the network. This leads to dire consequences such as the model not able to accurately classify whether there are walls present in front of it. This leads to road accidents and leads us to question the safety measures of self-driving cars.

Therefore, with the aid of our Neural Network verification toolbox, we would be able to identify how robust is the network from distortions. 
